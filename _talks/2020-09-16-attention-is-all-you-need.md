---
title: "Attention is All You Need"
collection: talks
type: "Talk"
permalink: /talks/2020-09-16-attention-is-all-you-need
venue: "CS 8803-DLT: Deep Learning for Text (Georgia Tech)"
date: 2020-09-16
location: "Atlanta, Georgia"
---

Introduces the Transformer, a sequence-to-sequence model by Vaswani et al. (2017) which uses the attention mechanism to learn dependencies between input and output tokens. Goes in-depth into the self-attention and multi-head attention mechanisms and discusses the advantages of the Transformer over recurrent neural networks.

[Talk URL](https://youtu.be/asl9WV7taNM)
