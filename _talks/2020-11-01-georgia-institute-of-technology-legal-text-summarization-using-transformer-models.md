---
title: 'Legal Text Summarization Using Transformer Models'
collection: talks
type: 'Talk'
permalink: /talks/2020-11-01-georgia-institute-of-technology-legal-text-summarization-using-transformer-models
venue: 'Georgia Institute of Technology'
date: '2020-11-01'
location: 'Atlanta, GA'
tagline: 'Presented work on a new transformer-based encoder-decoder architecture for abstractive legal text summarization, achieving state-of-the-art performance on the BIGPATENT dataset.'
---

This talk presents our work on a transformer-based encoder-decoder architecture for abstractive legal text summarization. Combines PEGASUS' (from Zhang et al. 2020) pre-training objective with Longformer's (from Beltagy et al. 2020) dilated attention mechanism to create a model that can handle extremely long input sequences to generate summaries of legal documents. Achieves state-of-the-art summarization performance on the BIGPATENT dataset.

Slides: [https://github.com/nimashoghi/dl-summarization](https://github.com/nimashoghi/dl-summarization)

