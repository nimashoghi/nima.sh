---
title: 'SmaQ: Smart quantization for DNN training by exploiting value clustering'
collection: publications
permalink: /publication/2021-SmaQ-Smart-quantization-for-DNN-training-by-exploiting-value-clustering
excerpt: 'Introduced SmaQ, a quantization scheme that leverages the normal distribution of neural network data structures to efficiently quantize them, addressing the memory bottleneck in single-machine training of deep networks.'
date: 2021
venue: 'IEEE Computer Architecture Letters'
paperurl: 'https://ieeexplore.ieee.org/abstract/document/9525237/'
authors: '<b>Nima Shoghi</b>, Andrei Bersatti, Moinuddin Qureshi, Hyesoon Kim'
citation: '<b>Nima Shoghi</b>, Andrei Bersatti, Moinuddin Qureshi, Hyesoon Kim, IEEE Computer Architecture Letters 20 (2), 126-129, 2021'
full_citation: '<b>Nima Shoghi</b>, Andrei Bersatti, Moinuddin Qureshi, Hyesoon Kim, IEEE Computer Architecture Letters 20 (2), 126-129, 2021'
---

This paper introduces Smart Quantization (SmaQ), a quantization scheme that exploits the observed normal distribution of various data structures used in neural networks to quantize them efficiently. By studying the characteristics of weight, gradient, feature map, gradient map, and optimizer state distributions for popular neural network architectures, the authors propose a dynamic quantization method that calculates the sampled mean and standard deviation of tensors to quantize each tensor, ultimately addressing the memory bottleneck in single-machine training of deep networks.

[Access paper here](https://ieeexplore.ieee.org/abstract/document/9525237/){:target="_blank"}
