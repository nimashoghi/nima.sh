---
title: "SmaQ: Smart Quantization for DNN Training by Exploiting Value Clustering"
collection: publications
permalink: /publication/2021-01-01-SmaQ-Smart-Quantization-for-DNN-Training-by-Exploiting-Value-Clustering
excerpt: 'Introduces Smart Quantization (SmaQ), a strategy that capitalizes on the normal distribution of neural network data structures, effectively reducing the required memory for deep neural network training on single machines by up to 6.7 times while preserving accuracy.'
date: 2021-01-01
venue: 'IEEE Computer Architecture Letters'
paperurl: 'https://doi.org/10.1109/LCA.2021.3108505'
citation: ' Nima Shoghi,  Andrei Bersatti,  Moinuddin Qureshi,  Hyesoon Kim, &quot;SmaQ: Smart Quantization for DNN Training by Exploiting Value Clustering.&quot; IEEE Computer Architecture Letters, 2021.'
---
Introduces Smart Quantization (SmaQ), a strategy that capitalizes on the normal distribution of neural network data structures, effectively reducing the required memory for deep neural network training on single machines by up to 6.7 times while preserving accuracy.

[Access paper here](https://doi.org/10.1109/LCA.2021.3108505){:target="_blank"}
