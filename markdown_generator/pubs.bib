@inproceedings{asgari2020pisces,
  title={PISCES: Power-Aware Implementation of {SLAM} by Customizing Efficient Sparse Algebra},
  author={Asgari, Bahar and Hadidi, Ramyad and Ghaleshahi, Nima Shoghi and Kim, Hyesoon},
  booktitle={2020 57th ACM/IEEE Design Automation Conference (DAC)},
  url={https://doi.org/10.1109/DAC18072.2020.9218550},
  pages={1--6},
  year={2020},
  organization={IEEE},
  abstract={A key real-time task in autonomous systems is simultaneous localization and mapping (SLAM). Although prior work has proposed hardware accelerators to process SLAM in real time, they paid less attention to power consumption. To be more power-efficient, we propose Pisces, which co-optimizes power consumption and latency by exploiting sparsity, a key characteristic of SLAM missed in prior work. By orchestrating sparse data, Pisces aligns correlated data and enables deterministic, one-time, and parallel accesses to the on-chip memory. Therefore, Pisces (i) eliminates unnecessary memory accesses and (ii) enables pipelined and parallel processing. Our FPGA implementation shows that Pisces consumes 2.5× less power and executes SLAM 7.4× faster than the state of the art.}
}

@inproceedings{ghalehshahi2019slam,
  title={SLAM Performance on Embedded Robots},
  author={Ghalehshahi, Nima Shoghi and Hadidi, Ramyad and Kim, Hyesoon},
  booktitle={Student Research Competition at Embedded System Week (SRC ESWEEK)},
  url={https://hparch.gatech.edu/papers/shoghi_src_esweek.pdf},
  year={2019},
  abstract={We explore whether it is possible to run the popular ORBSLAM2 algorithm (simultaneous localization and mapping) in real-time on the Raspberry Pi 3B+ for use in embedded robots. We use a modified version of ORB-SLAM2 on the Pi and a laptop to measure the performance and accuracy of the algorithm on the EuRoC MAV dataset. We see similar accuracy between the two machines, but the Pi is about 10 times slower. Finally, we explore optimizations that can be applied to speed up execution on the Pi. We conclude that with our optimizations, we can speed up ORB-SLAM2 by about 5 times with minor impact on accuracy, allowing us to run ORB-SLAM2 in real-time.}
}

@inproceedings{bersatti2020neural,
  title={Neural Network Weight Compression with NNW-BDI},
  author={Bersatti, Andrei and Shoghi Ghalehshahi, Nima and Kim, Hyesoon},
  booktitle={The International Symposium on Memory Systems},
  url={https://doi.org/10.1145/3422575.3422805},
  pages={335--340},
  year={2020},
  abstract={Memory is a scarce resource and increasingly so in the age of deep neural networks. Memory compression is a solution to the memory scarcity problem. This work proposes NNW-BDI, a scheme for compressing pretrained neural network weights. NNW-BDI is a variation to standard Base-Delta-Immediate [13] compression technique to make it a better fit for neural network weights, using techniques such as quantization, downscaling, randomized base selection, and base-delta-configuration adjustment. We evaluate our algorithm by compressing the weights of a MNIST classification network. Our evaluation shows that NNW-BDI reduces memory usage by up to 85\% percent without any reduction in inference accuracy.}
}

@inproceedings{hadidi2021quantifying,
  title={Quantifying the design-space tradeoffs in autonomous drones},
  author={Hadidi, Ramyad and Asgari, Bahar and Jijina, Sam and Amyette, Adriana and Shoghi, Nima and Kim, Hyesoon},
  booktitle={Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  url={https://doi.org/10.1145/3445814.3446721},
  pages={661--673},
  year={2021},
  abstract={With fully autonomous flight capabilities coupled with user-specific applications, drones, in particular quadcopter drones, are becoming prevalent solutions in myriad commercial and research contexts. However, autonomous drones must operate within constraints and design considerations that are quite different from any other compute-based agent. At any given time, a drone must arbitrate among its limited compute, energy, and electromechanical resources. Despite huge technological advances in this area, each of these problems has been approached in isolation and drone systems design-space tradeoffs are largely unknown. To address this knowledge gap, we formalize the fundamental drone subsystems and find how computations impact this design space. We present a design-space exploration of autonomous drone systems and quantify how we can provide productive solutions. As an example, we study widely used simultaneous localization and mapping (SLAM) on various platforms and demonstrate that optimizing SLAM on FPGA is more fruitful for the drones. Finally, to address the lack of publicly available experimental drones, we release our open-source drone that is customizable across the hardware-software stack.}
}

@inproceedings{jijina2020understanding,
  title={Understanding the Software and Hardware Stacks of a General-Purpose Cognitive Drone},
  author={Jijina, Sam and Amyette, Adriana and Shoghi, Nima and Hadidi, Ramyad and Kim, Hyesoon},
  booktitle={2020 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  url={https://doi.org/10.1109/ISPASS48437.2020.00036},
  pages={212--214},
  year={2020},
  organization={IEEE},
  abstract={Fully autonomous drones have a plethora of applications in the real world, from agriculture and communication to public services. With increasing attention, a new market segment has opened up for highly efficient drones. However, the deployment of efficient drones requires an in-depth analysis of several components spanning from hardware sensors to software stack. Specifically, to achieve high reliability, safety, and performance, the top concerns in the professional drone industry are characterizing underlying architecture and flight stack. In this paper, we characterize a widely-used open source flight stack, ArduCopter, to understand the performance requirements as a research community. Additionally, we study how area-specific applications affect flight stack. Our characterizations and benchmarks indicate that the drone flying range can be dramatically increased by optimizing the underlying flight controller software.}
}

@article{ghalehshahi2020secure,
  title={Secure Location-Aware Authentication and Communication for Intelligent Transportation Systems},
  author={Ghalehshahi, Nima Shoghi and Hadidi, Ramyad and Jaewon, Lee and Chen, Jun and Siqueria, Arthur and Rajan, Rahul and Dhawan, Shaan and Ghalehshahi, Pooya Shoghi and Kim, Hyesoon},
  journal={arXiv preprint arXiv:2011.08936},
  url={https://arxiv.org/abs/2011.08936},
  year={2020},
  abstract={Intelligent transportation systems (ITS) are expected to effectively create a stand-alone network for secure communication among autonomous agents. In such a dynamic and fast-changing network with high-speed agents, verifying the authenticity and integrity of messages while taking preventive action (e.g., applying brakes) within tens of milliseconds is one of the main challenges. In such a brief moment after receiving a message, the agent not only must verify the integrity and authenticity of the received message but also needs to perform extra computations to localize the sender of the message for taking appropriate action (e.g., an immediate stop warning from a vehicle in front vs. rear). In this paper, we present an inherently location-aware and lightweight authentication protocol by exploiting in situ visual localization (i.e., SLAM). In this protocol, each agent displays its public key using visual authentication beacons (e.g., QR codes). Thus, receiving agents not only can verify and authenticate the messages but also can easily localize the sender by keeping a shortlist of observed visual beacons within their visual localization system with no additional computation cost. Compared to prior work, our location-aware protocol is scalable, does not depend on any infrastructure, removes the high cost of post-message-delivery localization, and provides trustworthiness guarantees for information that are beyond the reach of each agent sensors.}
}

@article{shoghi2021smaq,
  title={SmaQ: Smart Quantization for DNN Training by Exploiting Value Clustering},
  author={Shoghi, Nima and Bersatti, Andrei and Qureshi, Moinuddin and Kim, Hyesoon},
  journal={IEEE Computer Architecture Letters},
  url={https://doi.org/10.1109/LCA.2021.3108505},
  volume={20},
  number={2},
  pages={126--129},
  year={2021},
  publisher={IEEE},
  abstract={Advancements in modern deep learning have shown that deeper networks with larger datasets can achieve state of the art results in many different tasks. As networks become deeper, the memory requirement of neural network training proves to be the primary bottleneck of single-machine training. In this letter, we first study the characteristics of neural network weight, gradient, feature map, gradient map, and optimizer state distributions for some popular neural network architectures. Our investigation shows that the majority of the data structures used by neural networks can have their value distributions be approximated with normal distributions. We then introduce Smart Quantization (SmaQ), a quantization scheme that exploits this observed normal distribution to quantize the data structures. Our dynamic quantization method calculates the sampled mean and standard deviation of tensors and quantizes each tensor element to 6 or 8 bits based on the z-score of that value. Our scheme reduces the memory usage during training by up to 6.7x with minor losses in accuracy.}
}

@article{kolluru2022transfer,
  title={Transfer learning using attentions across atomic systems with graph neural networks (TAAG)},
  author={Kolluru, Adeesh and Shoghi, Nima and Shuaibi, Muhammed and Goyal, Siddharth and Das, Abhishek and Zitnick, C Lawrence and Ulissi, Zachary},
  journal={The Journal of Chemical Physics},
  url={https://doi.org/10.1063/5.0088019},
  volume={156},
  number={18},
  year={2022},
  publisher={AIP Publishing},
  abstract={Recent advances in Graph Neural Networks (GNNs) have transformed the space of molecular and catalyst discovery. Despite the fact that the underlying physics across these domains remain the same, most prior work has focused on building domain-specific models either in small molecules or in materials. However, building large datasets across all domains is computationally expensive; therefore, the use of transfer learning (TL) to generalize to different domains is a promising but under-explored approach to this problem. To evaluate this hypothesis, we use a model that is pretrained on the Open Catalyst Dataset (OC20), and we study the model’s behavior when fine-tuned for a set of different datasets and tasks. This includes MD17, the *CO adsorbate dataset, and OC20 across different tasks. Through extensive TL experiments, we demonstrate that the initial layers of GNNs learn a more basic representation that is consistent across domains, whereas the final layers learn more task-specific features. Moreover, these well-known strategies show significant improvement over the non-pretrained models for in-domain tasks with improvements of 53\% and 17\% for the *CO dataset and across the Open Catalyst Project (OCP) task, respectively. TL approaches result in up to 4× speedup in model training depending on the target data and task. However, these do not perform well for the MD17 dataset, resulting in worse performance than the non-pretrained model for few molecules. Based on these observations, we propose transfer learning using attentions across atomic systems with graph Neural Networks (TAAG), an attention-based approach that adapts to prioritize and transfer important features from the interaction layers of GNNs. The proposed method outperforms the best TL approach for out-of-domain datasets, such as MD17, and gives a mean improvement of 6\% over a model trained from scratch.}
}

@article{kolluru2022open,
  title={Open Challenges in Developing Generalizable Large-Scale Machine-Learning Models for Catalyst Discovery},
  author={Kolluru, Adeesh and Shuaibi, Muhammed and Palizhati, Aini and Shoghi, Nima and Das, Abhishek and Wood, Brandon and Zitnick, C Lawrence and Kitchin, John R and Ulissi, Zachary W},
  journal={ACS Catalysis},
  url={https://doi.org/10.1021/acscatal.2c02291},
  volume={12},
  number={14},
  pages={8572--8581},
  year={2022},
  publisher={American Chemical Society},
  abstract={The development of machine-learned potentials for catalyst discovery has predominantly been focused on very specific chemistries and material compositions. While they are effective in interpolating between available materials, these approaches struggle to generalize across chemical space. The recent curation of large-scale catalyst data sets has offered the opportunity to build a universal machine-learning potential, spanning chemical and composition space. If accomplished, said potential could accelerate the catalyst discovery process across a variety of applications (CO2 reduction, NH3 production, etc.) without the additional specialized training efforts that are currently required. The release of the Open Catalyst 2020 Data set (OC20) has begun just that, pushing the heterogeneous catalysis and machine-learning communities toward building more accurate and robust models. In this Perspective, we discuss some of the challenges and findings of recent developments on OC20. We examine the performance of current models across different materials and adsorbates to identify notably underperforming subsets. We then discuss some of the modeling efforts surrounding energy conservation, approaches to finding and evaluating the local minima, and augmentation of off-equilibrium data. To complement the community’s ongoing developments, we end with an outlook to some of the important challenges that have yet to be thoroughly explored for large-scale catalyst discovery.}
}

@article{tran2023open,
  title={The Open Catalyst 2022 (OC22) dataset and challenges for oxide electrocatalysts},
  author={Tran, Richard and Lan, Janice and Shuaibi, Muhammed and Wood, Brandon M and Goyal, Siddharth and Das, Abhishek and Heras-Domingo, Javier and Kolluru, Adeesh and Rizvi, Ammar and Shoghi, Nima and others},
  journal={ACS Catalysis},
  volume={13},
  number={5},
  url={https://doi.org/10.48550/arXiv.2206.08917},
  pages={3066--3084},
  year={2023},
  publisher={American Chemical Society},
  abstract={The development of machine learning models for electrocatalysts requires a broad set of training data to enable their use across a wide variety of materials. One class of materials that currently lacks sufficient training data is oxides, which are critical for the development of OER catalysts. To address this, we developed the OC22 dataset, consisting of 62,331 DFT relaxations (~9,854,504 single point calculations) across a range of oxide materials, coverages, and adsorbates. We define generalized total energy tasks that enable property prediction beyond adsorption energies; we test baseline performance of several graph neural networks; and we provide pre-defined dataset splits to establish clear benchmarks for future efforts. In the most general task, GemNet-OC sees a ~36\% improvement in energy predictions when combining the chemically dissimilar OC20 and OC22 datasets via fine-tuning. Similarly, we achieved a ~19\% improvement in total energy predictions on OC20 and a ~9\% improvement in force predictions in OC22 when using joint training. We demonstrate the practical utility of a top performing model by capturing literature adsorption energies and important OER scaling relationships. We expect OC22 to provide an important benchmark for models seeking to incorporate intricate long-range electrostatic and magnetic interactions in oxide surfaces. Dataset and baseline models are open sourced, and a public leaderboard is available to encourage continued community developments on the total energy tasks and data.}
}

@inproceedings{HadidiGAK23,
  author       = {Ramyad Hadidi and
                  Nima Shoghi Ghaleshahi and
                  Bahar Asgari and
                  Hyesoon Kim},
  editor       = {Claudio A. Ardagna and
                  Feras M. Awaysheh and
                  Hongyi Bian and
                  Carl K. Chang and
                  Rong N. Chang and
                  Fl{\'{a}}via Coimbra Delicato and
                  Nirmit Desai and
                  Jing Fan and
                  Geoffrey C. Fox and
                  Andrzej Goscinski and
                  Zhi Jin and
                  Anna Kobusinska and
                  Omer F. Rana},
  title        = {Context-Aware Task Handling in Resource-Constrained Robots with Virtualization},
  booktitle    = {{IEEE} International Conference on Edge Computing and Communications,
                  {EDGE} 2023, Chicago, IL, USA, July 2-8, 2023},
  pages        = {255--261},
  publisher    = {{IEEE}},
  year         = {2023},
  url          = {https://doi.org/10.1109/EDGE60047.2023.00047},
  doi          = {10.1109/EDGE60047.2023.00047},
  timestamp    = {Mon, 11 Sep 2023 15:19:00 +0200},
  biburl       = {https://dblp.org/rec/conf/edge/HadidiGAK23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org},
  abstract     = {Intelligent mobile robots are critical in several scenarios. However, as their computational resources are limited, mobile robots struggle to handle several tasks concurrently while guaranteeing real timeliness. To address this challenge and improve the real-timeliness of critical tasks under resource constraints, we propose a fast context-aware task handling technique. To effectively handle tasks in real-time, our proposed context-aware technique comprises three main ingredients: (i) a dynamic time-sharing mechanism, coupled with (ii) an event-driven task scheduling using reactive programming paradigm to mindfully use the limited resources; and, (iii) a lightweight virtualized execution to easily integrate functionalities and their dependencies. We showcase our technique on a Raspberry-Pi-based robot with a variety of tasks such as Simultaneous localization and mapping (SLAM), sign detection, and speech recognition with a 42\% speedup in total execution time compared to the common Linux scheduler.}
}
